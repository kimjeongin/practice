import { Embeddings } from '@langchain/core/embeddings'
import fetch from 'node-fetch'
import { BaseServerConfig } from '@/shared/config/config-factory.js'
import { ModelInfo } from '@/domains/rag/core/types.js'
import { logger } from '@/shared/logger/index.js'

export interface OllamaModelConfig {
  modelId: string
  dimensions: number
  maxTokens: number
  description: string
  recommendedBatchSize?: number
}

export const AVAILABLE_OLLAMA_MODELS: Record<string, OllamaModelConfig> = {
  'nomic-embed-text': {
    modelId: 'nomic-embed-text',
    dimensions: 768,
    maxTokens: 2048,  // í† í° ë‹¨ìœ„: 2048 tokens (â‰ˆ 7168 characters with 3.5x ratio)
    description: 'Nomic Embed - Recommended general-purpose embedding model',
    recommendedBatchSize: 8,
  },
}

/**
 * LangChain í˜¸í™˜ Ollama ì„ë² ë”© í´ë˜ìŠ¤
 * ë¡œì»¬ Ollama ì„œë²„ì™€ í†µì‹ í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
 */
export class OllamaEmbeddings extends Embeddings {
  private baseUrl: string
  private model: string
  private requestOptions: Record<string, any>
  private cachedDimensions: number | null = null

  constructor(config: BaseServerConfig) {
    super({})
    this.baseUrl = config.ollamaBaseUrl || 'http://localhost:11434'
    this.model = config.embeddingModel
    this.requestOptions = {
      temperature: 0, // ì„ë² ë”©ì—ëŠ” deterministic ê²°ê³¼ í•„ìš”
      keep_alive: '1m', // ëª¨ë¸ì„ 1ë¶„ê°„ ë©”ëª¨ë¦¬ì— ìœ ì§€
    }
  }

  /**
   * ë‹¨ì¼ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ìƒì„±
   */
  async embedQuery(query: string): Promise<number[]> {
    try {
      const response = await fetch(`${this.baseUrl}/api/embeddings`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: this.model,
          prompt: query,
          ...this.requestOptions,
        }),
      })

      if (!response.ok) {
        throw new Error(`Ollama API error: ${response.status} ${response.statusText}`)
      }

      const data = (await response.json()) as { embedding: number[] }

      if (!data.embedding || !Array.isArray(data.embedding)) {
        throw new Error('Invalid embedding data received from Ollama')
      }

      // ì°¨ì› ìˆ˜ ìºì‹± (ì²« ë²ˆì§¸ ì„ë² ë”© ìƒì„± ì‹œ)
      if (this.cachedDimensions === null) {
        this.cachedDimensions = data.embedding.length
        logger.info(`ğŸ“Š Ollama model dimensions detected: ${this.cachedDimensions}`)
      }

      return data.embedding
    } catch (error) {
      logger.error('Error generating Ollama embedding for query:', error instanceof Error ? error : new Error(String(error)))
      throw error
    }
  }

  /**
   * ì—¬ëŸ¬ ë¬¸ì„œì— ëŒ€í•œ ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
   */
  async embedDocuments(documents: string[]): Promise<number[][]> {
    if (documents.length === 0) {
      return []
    }

    try {
      logger.info(`Generating embeddings for ${documents.length} documents...`)
      const embeddings: number[][] = []

      // OllamaëŠ” ë°°ì¹˜ ì„ë² ë”©ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìˆœì°¨ ì²˜ë¦¬
      // ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë³‘ë ¬ ì²˜ë¦¬ ì˜µì…˜ ì¶”ê°€ (ë™ì‹œ ì—°ê²° ì œí•œ)
      const concurrency = 3 // ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ

      for (let i = 0; i < documents.length; i += concurrency) {
        const batch = documents.slice(i, i + concurrency)
        const batchPromises = batch.map((doc) => this.embedQuery(doc))

        try {
          const batchEmbeddings = await Promise.all(batchPromises)
          embeddings.push(...batchEmbeddings)

          // ì§„í–‰ìƒí™© ë¡œê¹…
          if (i % (concurrency * 5) === 0) {
            logger.debug(
              `Progress: ${Math.min(i + concurrency, documents.length)}/${
                documents.length
              } embeddings generated`
            )
          }
        } catch (error) {
          logger.error(`Error in batch ${i}-${i + concurrency}:`, error instanceof Error ? error : new Error(String(error)))
          throw error
        }
      }

      logger.info(`Successfully generated ${embeddings.length} embeddings`)
      return embeddings
    } catch (error) {
      logger.error('Error generating Ollama embeddings for documents:', error instanceof Error ? error : new Error(String(error)))
      throw error
    }
  }

  /**
   * Ollama ì„œë²„ ìƒíƒœ í™•ì¸
   */
  async healthCheck(): Promise<boolean> {
    try {
      const controller = new AbortController()
      const timeoutId = setTimeout(() => controller.abort(), 5000)

      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
        signal: controller.signal,
      })

      clearTimeout(timeoutId)
      return response.ok
    } catch (error) {
      logger.warn('Ollama health check failed:', error instanceof Error ? error : new Error(String(error)))
      return false
    }
  }

  /**
   * ì§€ì •ëœ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
   */
  async isModelAvailable(): Promise<boolean> {
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
      })

      if (!response.ok) {
        return false
      }

      const data = (await response.json()) as { models: Array<{ name: string }> }
      return data.models.some(
        (model) => model.name === this.model || model.name.startsWith(this.model + ':')
      )
    } catch (error) {
      logger.warn('Error checking Ollama model availability:', error instanceof Error ? error : new Error(String(error)))
      return false
    }
  }

  /**
   * ëª¨ë¸ ì •ë³´ ë° ì„¤ì • ë°˜í™˜ (ModelInfo ì¸í„°í˜ì´ìŠ¤ í˜¸í™˜)
   */
  getModelInfo(): ModelInfo {
    try {
      return {
        name: this.model,
        service: 'ollama',
        dimensions: this.cachedDimensions || 768, // ìºì‹œëœ ì°¨ì› ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’
        model: this.model,
      }
    } catch (error) {
      logger.warn('Error getting model info:', error instanceof Error ? error : new Error(String(error)))
      return {
        name: this.model || 'unknown',
        service: 'ollama',
        dimensions: this.cachedDimensions || 768,
        model: this.model,
      }
    }
  }

  /**
   * ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜ (ë™ì  ê°ì§€ ë˜ëŠ” ìºì‹œëœ ê°’)
   */
  async getEmbeddingDimensions(): Promise<number> {
    // ì´ë¯¸ ìºì‹œëœ ì°¨ì› ìˆ˜ê°€ ìˆìœ¼ë©´ ë°˜í™˜
    if (this.cachedDimensions !== null) {
      return this.cachedDimensions
    }

    try {
      // í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¡œ ì„ë² ë”© ìƒì„±í•˜ì—¬ ì°¨ì› ìˆ˜ í™•ì¸
      const testEmbedding = await this.embedQuery('test')
      // embedQueryì—ì„œ ì´ë¯¸ ìºì‹±ë˜ë¯€ë¡œ ê¸¸ì´ë§Œ ë°˜í™˜
      return testEmbedding.length
    } catch (error) {
      logger.warn('Could not determine embedding dimensions, using default')
      // ê¸°ë³¸ê°’ ì„¤ì • ë° ìºì‹±
      this.cachedDimensions = 768
      return 768
    }
  }

  /**
   * Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
   */
  async getAvailableModels(): Promise<Record<string, any>> {
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
      })

      if (!response.ok) {
        throw new Error(`Failed to fetch models: ${response.status}`)
      }

      const data = (await response.json()) as {
        models: Array<{ name: string; size: number; modified_at: string }>
      }

      const modelMap: Record<string, any> = {}
      for (const model of data.models) {
        modelMap[model.name] = {
          name: model.name,
          size: model.size,
          modified_at: model.modified_at,
          description: `Ollama model: ${model.name}`,
        }
      }

      return modelMap
    } catch (error) {
      logger.warn('Could not fetch available models from Ollama:', error instanceof Error ? error : new Error(String(error)))
      // ê¸°ë³¸ê°’ ë°˜í™˜
      return {
        [this.model]: {
          name: this.model,
          description: `Current model: ${this.model}`,
        },
      }
    }
  }

  /**
   * ëª¨ë¸ ì „í™˜ (OllamaëŠ” ë‹¨ì¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ì´ë¯€ë¡œ ì œí•œì )
   */
  async switchModel(modelName: string): Promise<void> {
    if (modelName === this.model) {
      logger.info(`Already using model: ${modelName}`)
      return
    }

    // Ollamaì—ì„œ ëª¨ë¸ì´ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
    const availableModels = await this.getAvailableModels()
    if (!availableModels[modelName]) {
      throw new Error(
        `Model ${modelName} is not available in Ollama. Available models: ${Object.keys(
          availableModels
        ).join(', ')}`
      )
    }

    throw new Error(
      `Model switching not supported in current Ollama configuration. Current model: ${this.model}. To switch models, restart Ollama with the desired model.`
    )
  }

  /**
   * ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (Ollamaì—ì„œëŠ” `ollama pull` ëª…ë ¹ìœ¼ë¡œ ê´€ë¦¬)
   */
  async downloadModel(): Promise<void> {
    throw new Error(
      'Model downloading should be done directly through Ollama CLI using `ollama pull <model-name>`'
    )
  }

  /**
   * ìºì‹œ í†µê³„ (Ollama ì„œë²„ê°€ ìì²´ ê´€ë¦¬)
   */
  async getCacheStats(): Promise<any> {
    return {
      message: 'Cache statistics are managed by Ollama server directly',
      note: 'Use `ollama ps` command to see running models',
    }
  }

  /**
   * ë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥  (Ollama ì„œë²„ê°€ ìì²´ ê´€ë¦¬)
   */
  getDownloadProgress(): any {
    return {
      message: 'Download progress is managed by Ollama server directly',
      note: 'Use `ollama pull <model-name>` command to download models',
    }
  }

  /**
   * List available models with configuration
   */
  static getAvailableModels(): Record<string, OllamaModelConfig> {
    return AVAILABLE_OLLAMA_MODELS
  }

  /**
   * Get model configuration by name
   */
  static getModelConfig(modelName: string): OllamaModelConfig | null {
    // Try exact match first
    if (AVAILABLE_OLLAMA_MODELS[modelName]) {
      return AVAILABLE_OLLAMA_MODELS[modelName]
    }

    // Try partial matches for versioned models
    for (const [key, config] of Object.entries(AVAILABLE_OLLAMA_MODELS)) {
      const keyParts = key.split(':')
      const slashParts = key.split('/')
      
      if (keyParts[0] && modelName.includes(keyParts[0])) {
        return config
      }
      if (slashParts.length > 1 && slashParts[1] && modelName.includes(slashParts[1])) {
        return config
      }
    }

    return null
  }

  /**
   * Get dimensions for a specific model
   */
  static getModelDimensions(modelName: string): number {
    const config = OllamaEmbeddings.getModelConfig(modelName)
    return config?.dimensions || 768 // fallback to default
  }

  /**
   * Get recommended batch size for a specific model
   */
  static getModelBatchSize(modelName: string): number {
    const config = OllamaEmbeddings.getModelConfig(modelName)
    return config?.recommendedBatchSize || 8 // fallback to default
  }
}
