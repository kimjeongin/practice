import { existsSync, statSync, readdirSync } from 'fs'
import { join, dirname, extname, basename } from 'path'
import { createHash } from 'crypto'
import { IFileRepository } from '../repositories/document.js'
import { IChunkRepository } from '../repositories/chunk.js'
import { IVectorStoreService, IFileProcessingService } from '@/shared/types/interfaces.js'
import { FileMetadata } from '../core/models.js'
import { ServerConfig } from '@/shared/config/config-factory.js'
import { logger, startTiming } from '@/shared/logger/index.js'
import { withTimeout, withRetry } from '@/shared/utils/resilience.js'
import { EmbeddingMetadataService } from '../services/embedding-metadata-service.js'

export interface VectorDbSyncIssue {
  type: 'missing_file' | 'orphaned_vector' | 'hash_mismatch' | 'missing_chunks' | 'new_file' | 'dimension_mismatch' | 'model_incompatibility'
  filePath: string
  fileId?: string
  description: string
  severity: 'low' | 'medium' | 'high'
}

export interface VectorDbSyncReport {
  timestamp: Date
  totalFiles: number
  totalVectors: number
  totalChunks: number
  issues: VectorDbSyncIssue[]
  fixedIssues: VectorDbSyncIssue[]
  summary: {
    missingFiles: number
    orphanedVectors: number
    hashMismatches: number
    newFiles: number
    dimensionMismatches: number
    modelIncompatibilities: number
    totalIssues: number
  }
}

export interface VectorDbSyncOptions {
  autoFix: boolean
  deepScan: boolean
  includeNewFiles: boolean
  maxConcurrency: number
}

/**
 * ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ SQLite ê°„ ë™ê¸°í™” ê´€ë¦¬ì
 * í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ì¼ê´€ì„± ê²€ì‚¬ ë° ìë™ ë³µêµ¬ ê¸°ëŠ¥ ì œê³µ
 */
export class SyncManager {
  private documentsDirectory: string

  constructor(
    private fileRepository: IFileRepository,
    private chunkRepository: IChunkRepository,
    private vectorStoreService: IVectorStoreService,
    private config: ServerConfig,
    private fileProcessingService?: IFileProcessingService,
    private embeddingMetadataService?: EmbeddingMetadataService
  ) {
    this.documentsDirectory = config.documentsDir
  }

  /**
   * í”„ë¡œê·¸ë¨ ì‹œì‘ ì‹œ ì „ì²´ ë™ê¸°í™” ê²€ì‚¬ ë° ë³µêµ¬
   */
  async performStartupSync(
    options: Partial<VectorDbSyncOptions> = {}
  ): Promise<VectorDbSyncReport> {
    const endTiming = startTiming('startup_sync', { component: 'VectorDbSyncManager' })

    const opts: VectorDbSyncOptions = {
      autoFix: true,
      deepScan: true,
      includeNewFiles: true,
      maxConcurrency: 3,
      ...options,
    }

    try {
      logger.info('Starting comprehensive sync check on startup', opts)

      const report = await this.generateSyncReport(opts)

      if (opts.autoFix && report.issues.length > 0) {
        logger.info('Auto-fixing sync issues', { issueCount: report.issues.length })
        await this.fixSyncIssues(report.issues, opts)
      }

      // Auto-compact sparse index after sync if needed
      if (opts.autoFix && 'autoCompactIfNeeded' in this.vectorStoreService) {
        try {
          const compacted = await (this.vectorStoreService as any).autoCompactIfNeeded()
          if (compacted) {
            logger.info('Auto-compacted sparse vector index after sync')
          }
        } catch (error) {
          logger.warn(
            'Failed to auto-compact index after sync',
            error instanceof Error ? error : new Error(String(error))
          )
        }
      }

      // Regenerate embeddings for existing documents if needed
      await this.regenerateEmbeddingsIfNeeded()

      logger.info('Startup sync completed', {
        totalIssues: report.issues.length,
        fixedIssues: report.fixedIssues.length,
        summary: report.summary,
      })

      return report
    } catch (error) {
      logger.error('Startup sync failed', error instanceof Error ? error : new Error(String(error)))
      throw error
    } finally {
      endTiming()
    }
  }

  /**
   * ë™ê¸°í™” ìƒíƒœ ë³´ê³ ì„œ ìƒì„±
   */
  async generateSyncReport(options: VectorDbSyncOptions): Promise<VectorDbSyncReport> {
    const issues: VectorDbSyncIssue[] = []
    const fixedIssues: VectorDbSyncIssue[] = []

    // 1. ë°ì´í„°ë² ì´ìŠ¤ì˜ íŒŒì¼ë“¤ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    await this.checkMissingFiles(issues)

    // 2. íŒŒì¼ í•´ì‹œ ë³€ê²½ì‚¬í•­ í™•ì¸
    if (options.deepScan) {
      await this.checkHashMismatches(issues)
    }

    // 3. ìƒˆë¡œìš´ íŒŒì¼ ë°œê²¬
    if (options.includeNewFiles) {
      await this.checkNewFiles(issues)
    }

    // 4. ê³ ì•„ ë²¡í„° ë° ì²­í¬ í™•ì¸
    await this.checkOrphanedData(issues)

    // 5. ë²¡í„° ìŠ¤í† ì–´ì™€ ë°ì´í„°ë² ì´ìŠ¤ ê°„ ì¼ê´€ì„± í™•ì¸
    await this.checkVectorConsistency(issues)

    // 6. ì„ë² ë”© ëª¨ë¸ í˜¸í™˜ì„± ë° ì°¨ì› ê²€ì¦
    if (this.embeddingMetadataService) {
      await this.checkEmbeddingCompatibility(issues)
    }

    const allFiles = await this.fileRepository.getAllFiles()
    const totalVectors = await this.getTotalVectorCount()
    const totalChunks = await this.chunkRepository.getTotalChunkCount()

    return {
      timestamp: new Date(),
      totalFiles: allFiles.length,
      totalVectors,
      totalChunks,
      issues,
      fixedIssues,
      summary: {
        missingFiles: issues.filter((i) => i.type === 'missing_file').length,
        orphanedVectors: issues.filter((i) => i.type === 'orphaned_vector').length,
        hashMismatches: issues.filter((i) => i.type === 'hash_mismatch').length,
        newFiles: issues.filter((i) => i.type === 'new_file').length,
        dimensionMismatches: issues.filter((i) => i.type === 'dimension_mismatch').length,
        modelIncompatibilities: issues.filter((i) => i.type === 'model_incompatibility').length,
        totalIssues: issues.length,
      },
    }
  }

  /**
   * ëˆ„ë½ëœ íŒŒì¼ í™•ì¸ (DBì—ëŠ” ìˆì§€ë§Œ ì‹¤ì œ íŒŒì¼ì´ ì—†ìŒ)
   */
  private async checkMissingFiles(issues: VectorDbSyncIssue[]): Promise<void> {
    const allFiles = await this.fileRepository.getAllFiles()

    for (const file of allFiles) {
      if (!existsSync(file.path)) {
        issues.push({
          type: 'missing_file',
          filePath: file.path,
          fileId: file.id,
          description: `File exists in database but not on filesystem`,
          severity: 'high',
        })
      }
    }
  }

  /**
   * íŒŒì¼ í•´ì‹œ ë³€ê²½ì‚¬í•­ í™•ì¸
   */
  private async checkHashMismatches(issues: VectorDbSyncIssue[]): Promise<void> {
    const allFiles = await this.fileRepository.getAllFiles()

    for (const file of allFiles) {
      if (existsSync(file.path)) {
        try {
          const currentHash = await this.calculateFileHash(file.path)
          if (currentHash !== file.hash) {
            issues.push({
              type: 'hash_mismatch',
              filePath: file.path,
              fileId: file.id,
              description: `File content has changed (hash mismatch)`,
              severity: 'medium',
            })
          }
        } catch (error) {
          logger.warn('Failed to calculate hash for file', { filePath: file.path, error })
        }
      }
    }
  }

  /**
   * ìƒˆë¡œìš´ íŒŒì¼ ë°œê²¬ (íŒŒì¼ ì‹œìŠ¤í…œì—ëŠ” ìˆì§€ë§Œ DBì— ì—†ìŒ)
   */
  private async checkNewFiles(issues: VectorDbSyncIssue[]): Promise<void> {
    const supportedExtensions = ['.txt', '.md', '.pdf', '.csv', '.json']
    
    logger.info('ğŸ“ Scanning documents directory for new files', {
      directory: this.documentsDirectory,
      supportedExtensions
    })

    try {
      const initialCount = issues.length
      await this.scanDirectoryForNewFiles(this.documentsDirectory, supportedExtensions, issues)
      const newFileCount = issues.length - initialCount
      
      if (newFileCount > 0) {
        logger.info('ğŸ” New files discovered', {
          count: newFileCount,
          directory: this.documentsDirectory
        })
      } else {
        logger.info('âœ… No new files found', {
          directory: this.documentsDirectory
        })
      }
    } catch (error) {
      logger.warn('Failed to scan directory for new files', {
        error,
        directory: this.documentsDirectory,
      })
    }
  }

  /**
   * ë””ë ‰í† ë¦¬ ì¬ê·€ì  ìŠ¤ìº”í•˜ì—¬ ìƒˆ íŒŒì¼ ì°¾ê¸°
   */
  private async scanDirectoryForNewFiles(
    directory: string,
    supportedExtensions: string[],
    issues: VectorDbSyncIssue[]
  ): Promise<void> {
    if (!existsSync(directory)) {
      return
    }

    try {
      const entries = readdirSync(directory, { withFileTypes: true })

      for (const entry of entries) {
        const fullPath = join(directory, entry.name)

        if (entry.isDirectory()) {
          // ì¼ë¶€ ë””ë ‰í† ë¦¬ëŠ” ìŠ¤í‚µ (ì˜ˆ: node_modules, .git ë“±)
          if (!['node_modules', '.git', 'dist', 'coverage'].includes(entry.name)) {
            await this.scanDirectoryForNewFiles(fullPath, supportedExtensions, issues)
          }
        } else if (entry.isFile()) {
          const fileExt = extname(entry.name).toLowerCase()

          if (supportedExtensions.includes(fileExt)) {
            // ë°ì´í„°ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ì¸ì§€ í™•ì¸
            if (!(await this.fileRepository.getFileByPath(fullPath))) {
              const stats = statSync(fullPath)
              logger.debug('ğŸ“„ New file discovered', {
                filePath: fullPath,
                fileType: fileExt,
                size: stats.size,
                modifiedAt: stats.mtime
              })
              issues.push({
                type: 'new_file',
                filePath: fullPath,
                description: `New ${fileExt} file found (${stats.size} bytes)`,
                severity: 'low',
              })
            }
          }
        }
      }
    } catch (error) {
      logger.warn('Failed to read directory', { error, directory })
    }
  }

  /**
   * ê³ ì•„ ë°ì´í„° í™•ì¸
   */
  private async checkOrphanedData(issues: VectorDbSyncIssue[]): Promise<void> {
    // ë²¡í„° ìŠ¤í† ì–´ì—ëŠ” ìˆì§€ë§Œ ë°ì´í„°ë² ì´ìŠ¤ì— ì—†ëŠ” ë¬¸ì„œë“¤ í™•ì¸
    if ('getAllDocumentIds' in this.vectorStoreService) {
      const vectorDocIds = await (this.vectorStoreService as any).getAllDocumentIds()
      const dbFileIds = new Set((await this.fileRepository.getAllFiles()).map((f) => f.id))

      for (const docId of vectorDocIds) {
        // ë¬¸ì„œ IDì—ì„œ íŒŒì¼ ID ì¶”ì¶œ (êµ¬í˜„ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        const fileId = this.extractFileIdFromDocId(docId)
        if (fileId && !dbFileIds.has(fileId)) {
          issues.push({
            type: 'orphaned_vector',
            filePath: 'unknown',
            fileId,
            description: `Vector exists for deleted file: ${fileId}`,
            severity: 'medium',
          })
        }
      }
    }
  }

  /**
   * ë²¡í„° ìŠ¤í† ì–´ì™€ ë°ì´í„°ë² ì´ìŠ¤ ê°„ ì¼ê´€ì„± í™•ì¸
   */
  private async checkVectorConsistency(issues: VectorDbSyncIssue[]): Promise<void> {
    const allFiles = await this.fileRepository.getAllFiles()

    for (const file of allFiles) {
      const chunks = await this.chunkRepository.getChunksByFileId(file.id)

      if (chunks.length === 0) {
        issues.push({
          type: 'missing_chunks',
          filePath: file.path,
          fileId: file.id,
          description: `File exists but has no chunks in database`,
          severity: 'high',
        })
      }
    }
  }

  /**
   * ì„ë² ë”© ëª¨ë¸ í˜¸í™˜ì„± ë° ì°¨ì› ê²€ì¦
   */
  private async checkEmbeddingCompatibility(issues: VectorDbSyncIssue[]): Promise<void> {
    if (!this.embeddingMetadataService) {
      return
    }

    try {
      logger.info('ğŸ” Checking embedding model compatibility and dimensions')

      // Get current model information from vector store provider
      const vectorStoreProvider = (this.vectorStoreService as any).provider
      if (!vectorStoreProvider) {
        logger.warn('Vector store provider not available for compatibility check')
        return
      }

      // Check if provider has model info method
      let currentModelInfo
      if (typeof vectorStoreProvider.getModelInfo === 'function') {
        currentModelInfo = await vectorStoreProvider.getModelInfo()
      } else {
        // Fallback to config-based model info
        currentModelInfo = {
          name: this.config.embeddingModel || 'unknown',
          service: this.config.embeddingService || 'unknown',
          dimensions: this.config.embeddingDimensions || 384,
          model: this.config.embeddingModel || 'unknown',
        }
      }

      // Check compatibility with stored metadata
      const compatibilityResult = await this.embeddingMetadataService.checkModelCompatibility(
        this.config,
        currentModelInfo
      )

      if (!compatibilityResult.isCompatible) {
        issues.push({
          type: 'model_incompatibility',
          filePath: 'system',
          description: `Model incompatibility detected: ${compatibilityResult.issues.join(', ')}`,
          severity: 'high',
        })
      } else if (compatibilityResult.requiresReindexing) {
        issues.push({
          type: 'dimension_mismatch',
          filePath: 'system',
          description: `Model changes require reindexing: ${compatibilityResult.issues.join(', ')}`,
          severity: 'medium',
        })
      }

      // Additional dimension validation
      const totalVectors = await this.getTotalVectorCount()
      if (totalVectors > 0 && compatibilityResult.currentMetadata) {
        const expectedDimensions = compatibilityResult.currentMetadata.dimensions
        const currentDimensions = currentModelInfo.dimensions

        if (expectedDimensions !== currentDimensions) {
          issues.push({
            type: 'dimension_mismatch',
            filePath: 'system',
            description: `Vector dimension mismatch: stored vectors have ${expectedDimensions} dimensions but current model produces ${currentDimensions} dimensions`,
            severity: 'high',
          })
        }
      }

    } catch (error) {
      logger.warn('Failed to check embedding compatibility', {
        error: error instanceof Error ? error.message : String(error)
      })
    }
  }

  /**
   * ë™ê¸°í™” ë¬¸ì œ ìë™ ìˆ˜ì •
   */
  async fixSyncIssues(
    issues: VectorDbSyncIssue[],
    options: VectorDbSyncOptions
  ): Promise<VectorDbSyncIssue[]> {
    const fixedIssues: VectorDbSyncIssue[] = []

    for (const issue of issues) {
      try {
        switch (issue.type) {
          case 'missing_file':
            await this.fixMissingFile(issue)
            fixedIssues.push(issue)
            break

          case 'orphaned_vector':
            await this.fixOrphanedVector(issue)
            fixedIssues.push(issue)
            break

          case 'hash_mismatch':
            await this.fixHashMismatch(issue)
            fixedIssues.push(issue)
            break

          case 'new_file':
            await this.fixNewFile(issue)
            fixedIssues.push(issue)
            break

          case 'missing_chunks':
            await this.fixMissingChunks(issue)
            fixedIssues.push(issue)
            break

          case 'dimension_mismatch':
          case 'model_incompatibility':
            await this.fixModelIncompatibility(issue)
            fixedIssues.push(issue)
            break
        }
      } catch (error) {
        logger.error(
          'Failed to fix sync issue',
          error instanceof Error ? error : new Error(String(error))
        )
      }
    }

    return fixedIssues
  }

  /**
   * ëˆ„ë½ëœ íŒŒì¼ ë¬¸ì œ í•´ê²° (DBì™€ ë²¡í„°ì—ì„œ ì œê±°)
   */
  private async fixMissingFile(issue: VectorDbSyncIssue): Promise<void> {
    if (!issue.fileId) return

    logger.info('Fixing missing file issue', { filePath: issue.filePath, fileId: issue.fileId })

    // 1. ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ì œê±°
    await withTimeout(this.vectorStoreService.removeDocumentsByFileId(issue.fileId), {
      timeoutMs: 30000,
      operation: 'remove_orphaned_vectors',
    })

    // 2. ì²­í¬ ì œê±°
    await this.chunkRepository.deleteDocumentChunks(issue.fileId)

    // 3. íŒŒì¼ ë©”íƒ€ë°ì´í„° ì œê±°
    await this.fileRepository.deleteFile(issue.fileId)
  }

  /**
   * ê³ ì•„ ë²¡í„° ë¬¸ì œ í•´ê²°
   */
  private async fixOrphanedVector(issue: VectorDbSyncIssue): Promise<void> {
    if (!issue.fileId) return

    logger.info('Fixing orphaned vector issue', { fileId: issue.fileId })

    await withTimeout(this.vectorStoreService.removeDocumentsByFileId(issue.fileId), {
      timeoutMs: 30000,
      operation: 'remove_orphaned_vectors',
    })
  }

  /**
   * í•´ì‹œ ë¶ˆì¼ì¹˜ ë¬¸ì œ í•´ê²° (íŒŒì¼ ì¬ì¸ë±ì‹±)
   */
  private async fixHashMismatch(issue: VectorDbSyncIssue): Promise<void> {
    logger.info('Fixing hash mismatch issue', { filePath: issue.filePath })

    if (this.fileProcessingService) {
      await withTimeout(this.fileProcessingService.processFile(issue.filePath), {
        timeoutMs: 300000,
        operation: 'fix_hash_mismatch',
      })
    } else {
      logger.warn('FileProcessingService not available for fixing hash mismatch')
    }
  }

  /**
   * ìƒˆ íŒŒì¼ ë¬¸ì œ í•´ê²° (ì¸ë±ì‹± ì¶”ê°€)
   */
  private async fixNewFile(issue: VectorDbSyncIssue): Promise<void> {
    logger.info('ğŸ“„ Processing new file', { filePath: issue.filePath })

    try {
      if (!existsSync(issue.filePath)) {
        logger.warn('File does not exist, skipping', { filePath: issue.filePath })
        return
      }

      // 1ï¸âƒ£ Create file metadata
      const fileMetadata = await this.createFileMetadata(issue.filePath)
      
      // 2ï¸âƒ£ Insert file metadata into database
      const fileId = await this.fileRepository.insertFile(fileMetadata)
      logger.debug('File metadata inserted', { 
        filePath: issue.filePath, 
        fileId, 
        fileType: fileMetadata.fileType,
        size: fileMetadata.size
      })

      // 3ï¸âƒ£ Process file for embeddings if FileProcessingService is available
      if (this.fileProcessingService) {
        logger.info('ğŸ§  Starting document processing and embedding generation', { 
          filePath: issue.filePath,
          fileId 
        })
        
        await this.fileProcessingService.processFile(issue.filePath)
        
        // 4ï¸âƒ£ Validate that processing was successful
        const chunks = await this.chunkRepository.getChunksByFileId(fileId)
        logger.info('âœ… File processing completed', {
          filePath: issue.filePath,
          fileId,
          chunksCreated: chunks.length
        })
      } else {
        logger.warn('âš ï¸ FileProcessingService not available, file metadata only', { 
          filePath: issue.filePath 
        })
      }
    } catch (error) {
      logger.error(
        'âŒ Failed to process new file',
        error instanceof Error ? error : new Error(String(error)),
        { filePath: issue.filePath }
      )
      
      // Attempt to clean up partial state
      try {
        const existingFile = await this.fileRepository.getFileByPath(issue.filePath)
        if (existingFile) {
          await this.fileRepository.deleteFile(existingFile.id)
          logger.debug('Cleaned up partial file metadata', { filePath: issue.filePath })
        }
      } catch (cleanupError) {
        logger.warn('Failed to cleanup partial file state', { 
          filePath: issue.filePath,
          cleanupError: cleanupError instanceof Error ? cleanupError.message : String(cleanupError)
        })
      }
      
      throw error
    }
  }

  /**
   * Create file metadata from file system information
   */
  private async createFileMetadata(filePath: string): Promise<Omit<FileMetadata, 'id'>> {
    const stats = statSync(filePath)
    const hash = await this.calculateFileHash(filePath)
    const fileName = basename(filePath)
    const fileExtension = extname(fileName).toLowerCase().replace('.', '')

    return {
      path: filePath,
      name: fileName,
      size: stats.size,
      modifiedAt: stats.mtime,
      createdAt: stats.birthtime || stats.mtime, // fallback for systems without birthtime
      fileType: fileExtension || 'txt',
      hash
    }
  }

  /**
   * ëˆ„ë½ëœ ì²­í¬ ë¬¸ì œ í•´ê²°
   */
  private async fixMissingChunks(issue: VectorDbSyncIssue): Promise<void> {
    logger.info('Fixing missing chunks issue', { filePath: issue.filePath })

    if (this.fileProcessingService) {
      await withTimeout(this.fileProcessingService.processFile(issue.filePath), {
        timeoutMs: 300000,
        operation: 'fix_missing_chunks',
      })
    } else {
      logger.warn('FileProcessingService not available for fixing missing chunks')
    }
  }

  /**
   * ëª¨ë¸ ë¹„í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
   */
  private async fixModelIncompatibility(issue: VectorDbSyncIssue): Promise<void> {
    logger.info('ğŸ”„ Fixing model incompatibility issue', { 
      type: issue.type, 
      description: issue.description 
    })

    if (!this.embeddingMetadataService) {
      logger.warn('EmbeddingMetadataService not available for fixing model incompatibility')
      return
    }

    try {
      if (issue.type === 'model_incompatibility' || issue.type === 'dimension_mismatch') {
        // Check if auto migration is enabled
        if (!this.config.modelMigration?.enableAutoMigration) {
          logger.warn('âš ï¸ Model incompatibility detected but auto migration is disabled', {
            enableAutoMigration: this.config.modelMigration?.enableAutoMigration,
            issue: issue.description
          })
          return
        }

        logger.info('ğŸ—‘ï¸ Clearing existing vector store due to model incompatibility')
        
        // Clear vector store only if configured to do so
        if (this.config.modelMigration?.clearVectorsOnModelChange && 
            typeof (this.vectorStoreService as any).provider?.clearAll === 'function') {
          await (this.vectorStoreService as any).provider.clearAll()
        }

        // Force migration to new model by deactivating old metadata
        await this.embeddingMetadataService.forceMigration()

        // Re-process all existing files to regenerate embeddings with new model
        if (this.fileProcessingService) {
          logger.info('ğŸ”„ Regenerating all embeddings with new model configuration')
          const allFiles = await this.fileRepository.getAllFiles()
          
          for (const file of allFiles) {
            try {
              if (existsSync(file.path)) {
                logger.info(`ğŸ“„ Reprocessing file: ${file.name}`)
                await this.fileProcessingService.processFile(file.path)
              } else {
                logger.warn(`ğŸ“„ File not found, skipping: ${file.path}`)
              }
            } catch (fileError) {
              logger.error(
                `âŒ Failed to reprocess file ${file.name}:`,
                fileError instanceof Error ? fileError : new Error(String(fileError))
              )
            }
          }

          logger.info('âœ… Model migration completed - all embeddings regenerated')
        } else {
          logger.warn('âš ï¸ FileProcessingService not available - embeddings not regenerated')
        }
      }
    } catch (error) {
      logger.error(
        `âŒ Failed to fix model incompatibility: ${issue.type} - ${issue.description}`,
        error instanceof Error ? error : new Error(String(error))
      )
      throw error
    }
  }

  /**
   * íŒŒì¼ í•´ì‹œ ê³„ì‚°
   */
  private async calculateFileHash(filePath: string): Promise<string> {
    const fs = await import('fs')
    const data = await fs.promises.readFile(filePath)
    return createHash('sha256').update(data).digest('hex')
  }

  /**
   * ë¬¸ì„œ IDì—ì„œ íŒŒì¼ ID ì¶”ì¶œ (êµ¬í˜„ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
   */
  private extractFileIdFromDocId(docId: string): string | null {
    // ì˜ˆ: "chunk_fileId_index" í˜•íƒœì—ì„œ fileId ì¶”ì¶œ
    const match = docId.match(/^(?:chunk_)?([^_]+)_\d+$/)
    return match && match[1] ? match[1] : null
  }

  /**
   * ì „ì²´ ë²¡í„° ìˆ˜ ì¡°íšŒ
   */
  private async getTotalVectorCount(): Promise<number> {
    if ('getDocumentCount' in this.vectorStoreService) {
      return await (this.vectorStoreService as any).getDocumentCount()
    }
    return 0
  }

  /**
   * ê°•ì œ ë™ê¸°í™” (ì „ì²´ ì¬êµ¬ì„±)
   */
  async forceSync(): Promise<VectorDbSyncReport> {
    logger.info('Starting force synchronization')

    // 1. ëª¨ë“  ë²¡í„° ë° ì²­í¬ ì‚­ì œ
    await this.vectorStoreService.removeAllDocuments()
    await this.chunkRepository.deleteAllDocumentChunks()

    // 2. íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬
    const allFiles = await this.fileRepository.getAllFiles()
    for (const file of allFiles) {
      if (!existsSync(file.path)) {
        await this.fileRepository.deleteFile(file.id)
      }
    }

    // 3. ì „ì²´ ì¬ì¸ë±ì‹± í•„ìš” (FileProcessingServiceì— ìœ„ì„)
    logger.info('Force sync completed - full reindexing required')

    return this.generateSyncReport({
      autoFix: false,
      deepScan: true,
      includeNewFiles: true,
      maxConcurrency: 3,
    })
  }

  /**
   * Regenerate embeddings for existing documents if the vector store has documents but no embeddings
   */
  private async regenerateEmbeddingsIfNeeded(): Promise<void> {
    try {
      // Get document and vector counts
      const totalChunks = await this.chunkRepository.getTotalChunkCount()
      const vectorCount = this.vectorStoreService.getDocumentCount?.() || 0
      
      logger.info('Checking embedding regeneration need', {
        totalChunks,
        vectorCount,
      })

      // If we have chunks but no vectors, we need to regenerate embeddings
      if (totalChunks > 0 && vectorCount === 0) {
        logger.info('ğŸ”„ Embedding regeneration needed - chunks exist but no vectors found')
        
        // Check if the vector store has a regeneration method
        const vectorStoreService = this.vectorStoreService as any
        if (vectorStoreService.provider && typeof vectorStoreService.provider.regenerateEmbeddings === 'function') {
          
          // Load all chunks from database and create documents for the vector store
          const allFiles = await this.fileRepository.getAllFiles()
          
          for (const file of allFiles) {
            try {
              const chunks = await this.chunkRepository.getChunksByFileId(file.id)
              if (chunks.length === 0) continue

              // Create documents for embedding regeneration
              const documents = chunks.map((chunk) => ({
                id: chunk.id,
                content: chunk.content,
                metadata: {
                  fileId: file.id,
                  fileName: file.name,
                  fileType: file.fileType,
                  chunkIndex: chunk.chunkIndex,
                  source: { filename: file.name },
                },
              }))

              // Add documents to vector store (this will regenerate embeddings)
              await vectorStoreService.provider.addDocuments(documents)
              
              logger.info(`âœ… Regenerated embeddings for ${chunks.length} chunks from ${file.name}`)
              
            } catch (error) {
              logger.error(`âŒ Failed to regenerate embeddings for file ${file.name}:`, error instanceof Error ? error : new Error(String(error)))
            }
          }

          const finalVectorCount = vectorStoreService.provider.getVectorCount?.() || 0
          logger.info(`ğŸ‰ Embedding regeneration completed`, {
            totalChunks,
            finalVectorCount,
          })

        } else {
          logger.warn('âš ï¸ Vector store does not support embedding regeneration')
        }
      } else if (vectorCount > 0) {
        logger.info('âœ… Embeddings already exist, regeneration not needed', {
          totalChunks,
          vectorCount,
        })
      } else {
        logger.info('ğŸ“„ No documents found for embedding regeneration')
      }

    } catch (error) {
      logger.error('âŒ Failed to check/regenerate embeddings:', error instanceof Error ? error : new Error(String(error)))
      // Don't throw - this is not critical for server startup
    }
  }
}
