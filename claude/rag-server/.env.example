# ====================================================================
# RAG MCP Server Configuration - Local Setup
# ====================================================================

# Core Configuration
NODE_ENV=development
LOG_LEVEL=info

# Database Configuration (Local SQLite)
DATABASE_PATH=./data/rag.db
DATA_DIR=./data

# Vector Store Configuration (Local FAISS)
# No remote dependencies required - everything runs locally!

# Embedding Service Configuration
EMBEDDING_SERVICE=ollama  # Options: openai, ollama
EMBEDDING_BATCH_SIZE=10   # Reduced for local processing
EMBEDDING_DIMENSIONS=768  # Auto-detected from Ollama model

# Ollama Configuration (Local Embedding Server)
OLLAMA_BASE_URL=http://localhost:11434
EMBEDDING_MODEL=nomic-embed-text  # Recommended: fast and efficient

# Alternative Ollama Models:
# EMBEDDING_MODEL=all-minilm   # Smaller, faster (384 dimensions)
# EMBEDDING_MODEL=mxbai-embed-large  # Larger, more accurate (1024 dimensions)

# Document Processing Configuration
CHUNK_SIZE=1024
CHUNK_OVERLAP=50        # Increased for better context preservation
SIMILARITY_TOP_K=5
SIMILARITY_THRESHOLD=0.6  # Lowered for more inclusive results

# Performance Tuning (Optimized for Local)
MAX_CONCURRENT_EMBEDDINGS=3  # Conservative for local CPU
EMBEDDING_CACHE_SIZE=1000
FAISS_INDEX_TYPE=Flat       # Simple and reliable for small datasets

# Optional: OpenAI Configuration (if you prefer cloud embeddings)
# EMBEDDING_SERVICE=openai
# OPENAI_API_KEY=your_openai_api_key_here
# EMBEDDING_MODEL=text-embedding-3-small
# EMBEDDING_DIMENSIONS=1536

# Legacy ChromaDB Support (Optional - not used by default)
# CHROMA_SERVER_URL=http://localhost:8000
# CHROMA_COLLECTION_NAME=rag_documents

# ====================================================================
# Quick Setup Instructions:
# 1. Install Ollama: https://ollama.com
# 2. Pull embedding model: ollama pull nomic-embed-text
# 3. Start server: pnpm dev
# ====================================================================